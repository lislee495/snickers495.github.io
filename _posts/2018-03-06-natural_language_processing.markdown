---
layout: post
title:      "Natural Language Processing "
date:       2018-03-06 16:53:05 +0000
permalink:  natural_language_processing
---


As a writer and artist, I've felt particularly drawn to the concepts and possibilities surrounding natural language processing. 

Language is a core part of the human experience. Broken down, it tells us about who we are, where we came from, how we grew up. Take, for example, the [NYT quiz](link) about dialects. Somehow, knowing what a kitty-corner is or saying "merry" different from "marry" can say something about where you came from, and what kind of life experiences you might have had. 

Language grows even more fundamental when understood through the lens of critical theory. Language creates meaning—in essence, it creates the whole universe. _We_ are made through words. It is what allows us to separate abstract from the absolute, birthing intanglible concepts like _liberty_, or _injustice_ that can only be conveyed through symbols. 

Similar too, ideas like race or gender are upheld through language. Language betrays our biases just as it creates and perpetuates them. That's why issues like how the media protrays the deaths of black children or how people talk about rape culture are so important. Those simple distinctions in word choice and tone, so apparently harmless, become the barriers that surround us. They become how we think, how we see, how we act. "Black man" rather than "teen" becomes the social reality of young black children; so do slurs against women and other oppressed people. 

## Rise of NLP

Language has largely been an issue in computer science. Unlike numbers, it's rarely precise. Even grammar, while  generalized into rules, has nearly countless edge cases. How do we turn these 0's and 1's on a screen into meaningful words, and how do we link those words into meaningful sentences and then paragraphs? The computer has no concept of the word "though" or "love," and it may not ever understand those concepts, but understanding language means that it should know more of how it's used in relation to other words. 

To the frustration of computers and programmers, language is also fluid and ever-evolving. From text speak to today's meme culture, the ways of reading a sentence have changed. A meme by itself has no meaning—but repetition and an agreed upon interpretation gives it one. And that interpretation can be immediately reinforced, built upon, or discarded by anyone, which is what makes memes fun.

The human ability to understand language was so fundamental that it formed the basis of the Turing Test. This test, formulated in the 1950's by scientist Alan Turing in his seminal essay, "Computing Machinery and Intelligence," asserted that a machine could be considered intelligent if a human could carry a conversation with it without the human recognizing it was a machine. Since then, advanced language abilities have been inextricable in our portrayals of advanced technology, like in “2001: A Space Odyssey” or in recent "Black Mirror" episodes. 

Because of the ambiguity of language, Natural Language Processing (or NLP) only really began to take shape with the rise of machine learning. Machine learning refers to the use of real-world and human-coded traits to train computers on how to identify patterns and make accurate predictions. Machine learning, in this case, means the computer and the developers create algorithms that reflect how language is actually used, rather than how grammar dictates. From this, machines are able to create better models of speech over the oft-ignored static rules of grammar. 

Another part of the NLP revolution has been the rise of Big Data and unstructured data. Big unstructured datasets provide the basis for the computer's models, which become more accurate with larger amounts of data. The importance of these datasets being "unstructured" is that then the algorithms can extrapolate the intuitive structure or inherent patterns that emerge out of the data, superceding any rules imposed on the sets by humans. Essentially, from the chaos of our emails, text messages, and phone calls, patterns naturally arise. 

Given that the primary way we interact with the web is through text, it makes sense that so many players are so invested in the evolution of this field. Google and Stanford both have their own NLP software, which are tailored towards different goals. But a few commonalities is that these algorithms can analyze the "sentiment" or tone of the text, decipher parts of the sentence, classify named entities (like groups of people, cities, etc.), and create parse trees, which are like sentence diagrams. 

## Dreams 
The utility of NLP is nearly endless. Here are just a few examples of the possible uses of NLP:

* **Translation**: NLP offers a  way to translate text and speech into different languages. The difficulty of translation is understanding the exact meaning of a text and how to convey that adequately in another language. Google, in particular, excels at translation because its ability of look at millions of webpages to see how one language expresses something meaningfully.
* **Summarization**: is the extraction of the central ideas and themes of a text, while ignoring other information. This also works with **auto-tagging** which tags the central keywords of a text. 
* **Sentiment analysis**: is the central focus of the Stanford NLP, which identifies how positive or negative a text is. Marketers use this to analyze brand strategies, while product deparmtnets can use it to identify problem areas. 
* **Speech processing**: is what allows Siri, Google Home, and Alexa to understand customers' commands to buy groceries, set up a meeting, or call a friend. 

Another dream of NLP is that if we can see how we speak, how our inherent biases come out, then we may be able to correct ourselves. Understanding the ways we talk about the news or groups of people can make us think about how to do better. 

## Nightmares 

Despite all these benefits, NLP has obvious risks, though they're not inherent in its process. The bigger issue is what all this information and data will be done in the hands of others. From the minute that any text flows out of our fingers, our words are no longer necessarily our own. They belong to enormous datasets that themselves belong to enormous corporations, which may not have our interests at heart. 

The use of NLP has led to the rise of more aggressive targeted marketing, tailoring ads and even content to suit our preferences. When we only see what others think we want to see, the internet becomes less like a shared space, and more like various gated playgrounds. Consumers also have to think about how much they want their information to be used for the benefit of these organizations, how much they want their identities and words digitized as data. 

In 2014, the Turing test was officially passed. A program called Eugene Goostman, simulating a 13-year-old Ukrainian boy, passed the bar. Since then, and even before, there's been a proliferation of chat bots and fake people. It's becoming easier to disseminate fake information, especially if machines can use just a few keywords to aggregate a story or mimick a person's writing. When all we have is a name to go on for the authenticity of information, how do we judge what is real or not when everything can be mimicked and warped? 

   
